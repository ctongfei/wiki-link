The dataset is produced in several stages.  At the moment this includes: retrieval and processing.

# Retrieval

  To retrieve the webpages:

    > cd retrieve/; ./retrieve.sh /path/to/original/wiki-link-data

  This will result in several new files in retrieve/:
    - wiki-link-urls.dat -- a file consisting of one URL per line which was used by retrieve.sh
    - logs/ -- the output of wget for each downloaded file
    - pages/ -- the files which were downloaded
      - 000000/
        - 0
        - 1
        - 2
        - ...
      - 000001/
        - ...
      - ....

# Processing

  To process the downloaded files:

    > cd process/; mvn scala:run

